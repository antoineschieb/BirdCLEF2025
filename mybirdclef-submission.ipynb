{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac0acf54",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-02T08:05:38.838803Z",
     "iopub.status.busy": "2025-05-02T08:05:38.838493Z",
     "iopub.status.idle": "2025-05-02T08:05:51.174513Z",
     "shell.execute_reply": "2025-05-02T08:05:51.173289Z"
    },
    "papermill": {
     "duration": 12.34094,
     "end_time": "2025-05-02T08:05:51.176374",
     "exception": false,
     "start_time": "2025-05-02T08:05:38.835434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aschieb\\AppData\\Local\\Temp\\ipykernel_3152\\1754147197.py:63: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  predictions = pd.concat([predictions, new_row], axis=0, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    row_id       1139490       1192948       1194042  \\\n",
      "0    H02_20230420_074000_5  2.438436e-07  1.224529e-07  1.939583e-05   \n",
      "1   H02_20230420_074000_10  1.171384e-07  6.264505e-08  1.262106e-05   \n",
      "2   H02_20230420_074000_15  1.978547e-06  8.670925e-07  4.050570e-05   \n",
      "3   H02_20230420_074000_20  3.323807e-07  2.009233e-07  1.763334e-05   \n",
      "4   H02_20230420_074000_25  4.686690e-07  1.804626e-07  3.466604e-05   \n",
      "5   H02_20230420_074000_30  1.535506e-07  8.311643e-08  1.507958e-05   \n",
      "6   H02_20230420_074000_35  1.053030e-07  5.812122e-08  1.234698e-05   \n",
      "7   H02_20230420_074000_40  6.615792e-08  4.936289e-08  1.102239e-05   \n",
      "8   H02_20230420_074000_45  5.401632e-08  3.255416e-08  8.801645e-06   \n",
      "9   H02_20230420_074000_50  1.031015e-07  6.489057e-08  1.174515e-05   \n",
      "10  H02_20230420_074000_55  1.367797e-07  7.211742e-08  1.356830e-05   \n",
      "11  H02_20230420_074000_60  9.989711e-08  5.658960e-08  1.051624e-05   \n",
      "12   H02_20230420_112000_5  3.774606e-08  7.500735e-07  1.248645e-05   \n",
      "13  H02_20230420_112000_10  1.818499e-06  2.344824e-05  1.366836e-04   \n",
      "14  H02_20230420_112000_15  1.182054e-06  6.694153e-06  4.808081e-05   \n",
      "15  H02_20230420_112000_20  3.303319e-07  6.500467e-06  1.415417e-05   \n",
      "16  H02_20230420_112000_25  2.088142e-08  8.501933e-07  8.270687e-06   \n",
      "17  H02_20230420_112000_30  1.301673e-06  1.304384e-05  4.394156e-05   \n",
      "18  H02_20230420_112000_35  1.097443e-06  4.902180e-06  3.747338e-05   \n",
      "19  H02_20230420_112000_40  5.258259e-07  6.151114e-06  5.613715e-05   \n",
      "20  H02_20230420_112000_45  6.207375e-07  7.313480e-06  2.739824e-05   \n",
      "21  H02_20230420_112000_50  7.119626e-07  1.108753e-05  4.829455e-05   \n",
      "22  H02_20230420_112000_55  4.787906e-08  1.041060e-06  1.668066e-05   \n",
      "23  H02_20230420_112000_60  6.984165e-06  1.090052e-05  1.589032e-04   \n",
      "24   H02_20230420_154500_5  4.485079e-06  3.713328e-05  5.442728e-05   \n",
      "25  H02_20230420_154500_10  1.724768e-08  5.113934e-07  3.332834e-06   \n",
      "26  H02_20230420_154500_15  9.825726e-06  3.139699e-05  3.365021e-05   \n",
      "27  H02_20230420_154500_20  1.352346e-06  1.217358e-06  3.240795e-06   \n",
      "28  H02_20230420_154500_25  5.588702e-08  1.633506e-07  7.582401e-07   \n",
      "29  H02_20230420_154500_30  1.375621e-07  6.611691e-07  1.976759e-06   \n",
      "30  H02_20230420_154500_35  1.106737e-07  7.663649e-07  6.661368e-07   \n",
      "31  H02_20230420_154500_40  4.451546e-08  5.441432e-07  2.208503e-06   \n",
      "32  H02_20230420_154500_45  1.616294e-06  4.118404e-06  1.337728e-05   \n",
      "33  H02_20230420_154500_50  2.891424e-06  1.225179e-05  1.626678e-05   \n",
      "34  H02_20230420_154500_55  2.461272e-06  6.125268e-06  1.693208e-05   \n",
      "35  H02_20230420_154500_60  8.855288e-07  7.342934e-06  1.526591e-05   \n",
      "\n",
      "      126247   1346504        134933    135045       1462711       1462737  \\\n",
      "0   0.000013  0.000157  4.178130e-05  0.004198  2.348532e-07  9.536739e-06   \n",
      "1   0.000007  0.000092  1.956021e-05  0.003936  1.091238e-07  7.038619e-06   \n",
      "2   0.000116  0.000197  7.025232e-05  0.002563  2.124595e-06  7.818639e-05   \n",
      "3   0.000015  0.000065  1.469954e-05  0.002904  3.243156e-07  3.090586e-05   \n",
      "4   0.000035  0.000379  1.175870e-04  0.003535  4.682705e-07  1.589051e-05   \n",
      "5   0.000008  0.000107  2.778923e-05  0.004142  1.439455e-07  8.512701e-06   \n",
      "6   0.000006  0.000093  2.006660e-05  0.004144  9.595664e-08  5.923083e-06   \n",
      "7   0.000003  0.000068  1.251103e-05  0.004511  5.401692e-08  3.911904e-06   \n",
      "8   0.000003  0.000062  9.979012e-06  0.003763  4.611338e-08  4.167042e-06   \n",
      "9   0.000005  0.000071  1.310123e-05  0.004021  8.933106e-08  6.519931e-06   \n",
      "10  0.000008  0.000102  1.998920e-05  0.003920  1.275765e-07  7.774423e-06   \n",
      "11  0.000006  0.000060  9.740431e-06  0.003152  9.253161e-08  9.361011e-06   \n",
      "12  0.000001  0.000031  4.511700e-06  0.005223  2.839034e-08  3.956562e-07   \n",
      "13  0.000022  0.000239  6.823400e-05  0.007428  1.550296e-06  1.348510e-05   \n",
      "14  0.000020  0.000110  1.906344e-05  0.004441  9.789948e-07  1.167342e-05   \n",
      "15  0.000013  0.000051  5.317886e-06  0.002477  3.324903e-07  3.408752e-06   \n",
      "16  0.000002  0.000018  1.519222e-06  0.002677  2.242152e-08  4.344525e-07   \n",
      "17  0.000034  0.000101  1.076785e-05  0.002815  1.403294e-06  1.895150e-05   \n",
      "18  0.000019  0.000102  1.598692e-05  0.003835  9.058165e-07  1.435760e-05   \n",
      "19  0.000010  0.000108  2.440942e-05  0.006262  4.306561e-07  3.262517e-06   \n",
      "20  0.000016  0.000082  1.620755e-05  0.003922  5.606698e-07  3.294592e-06   \n",
      "21  0.000012  0.000170  6.591329e-05  0.006338  5.702000e-07  1.590587e-06   \n",
      "22  0.000002  0.000039  8.440353e-06  0.005890  3.640724e-08  2.347003e-07   \n",
      "23  0.000054  0.000382  1.627285e-04  0.007182  5.305824e-06  4.798080e-05   \n",
      "24  0.000056  0.000346  1.204993e-04  0.005223  3.478253e-06  9.510589e-06   \n",
      "25  0.000001  0.000011  9.987267e-07  0.001989  1.716223e-08  2.041170e-07   \n",
      "26  0.000134  0.000191  4.211487e-05  0.001943  1.028129e-05  4.493458e-05   \n",
      "27  0.000078  0.000005  1.991630e-06  0.001898  1.267901e-06  2.988962e-05   \n",
      "28  0.000009  0.000003  1.184980e-07  0.000740  6.637140e-08  8.502652e-06   \n",
      "29  0.000008  0.000019  6.696994e-07  0.001248  1.307691e-07  6.261606e-06   \n",
      "30  0.000011  0.000007  1.671877e-07  0.000632  1.236806e-07  8.140926e-06   \n",
      "31  0.000004  0.000012  4.923413e-07  0.001279  4.576719e-08  1.226854e-06   \n",
      "32  0.000051  0.000047  5.599208e-06  0.001998  1.668573e-06  2.536383e-05   \n",
      "33  0.000067  0.000085  1.054510e-05  0.001867  2.990868e-06  2.741190e-05   \n",
      "34  0.000074  0.000064  6.358092e-06  0.001494  2.772989e-06  2.628329e-05   \n",
      "35  0.000030  0.000060  6.203009e-06  0.001794  1.037883e-06  8.468584e-06   \n",
      "\n",
      "    ...   yebfly1   yebsee1   yecspi2   yectyr1   yehbla2   yehcar1   yelori1  \\\n",
      "0   ...  0.007972  0.002489  0.029912  0.001094  0.006499  0.013383  0.002235   \n",
      "1   ...  0.008446  0.002509  0.033353  0.000854  0.007195  0.014223  0.002155   \n",
      "2   ...  0.005853  0.002384  0.019529  0.001648  0.005721  0.010406  0.002554   \n",
      "3   ...  0.007544  0.003189  0.028983  0.001025  0.008042  0.010654  0.002483   \n",
      "4   ...  0.005767  0.002153  0.017620  0.001130  0.004580  0.009929  0.002000   \n",
      "5   ...  0.008026  0.002810  0.031630  0.000982  0.006945  0.011836  0.001981   \n",
      "6   ...  0.008729  0.002614  0.033599  0.000833  0.007181  0.014124  0.002082   \n",
      "7   ...  0.010766  0.003182  0.031330  0.000688  0.008320  0.013863  0.002036   \n",
      "8   ...  0.009699  0.002483  0.033140  0.000634  0.007579  0.015842  0.002186   \n",
      "9   ...  0.009644  0.002822  0.033929  0.000755  0.008056  0.015289  0.002294   \n",
      "10  ...  0.008685  0.002382  0.032287  0.000852  0.007136  0.015274  0.002339   \n",
      "11  ...  0.008613  0.002453  0.032043  0.000711  0.007666  0.014499  0.002393   \n",
      "12  ...  0.028744  0.008399  0.006658  0.000786  0.013225  0.011622  0.001592   \n",
      "13  ...  0.036661  0.009043  0.006097  0.001918  0.012137  0.010393  0.002770   \n",
      "14  ...  0.014117  0.013643  0.013738  0.001347  0.015226  0.010901  0.002795   \n",
      "15  ...  0.014955  0.020691  0.005825  0.001367  0.012760  0.008761  0.002865   \n",
      "16  ...  0.024403  0.011618  0.004187  0.000793  0.011927  0.010986  0.001779   \n",
      "17  ...  0.013265  0.019412  0.008619  0.001443  0.013616  0.009799  0.003178   \n",
      "18  ...  0.012259  0.015114  0.016107  0.001172  0.015333  0.009825  0.002630   \n",
      "19  ...  0.023702  0.008652  0.007635  0.001692  0.013056  0.013301  0.002946   \n",
      "20  ...  0.014533  0.010226  0.007156  0.002270  0.011039  0.012011  0.004242   \n",
      "21  ...  0.023715  0.006285  0.004308  0.003213  0.009212  0.012054  0.004220   \n",
      "22  ...  0.025772  0.005090  0.004708  0.001252  0.010263  0.013737  0.002249   \n",
      "23  ...  0.011696  0.007796  0.018656  0.002288  0.011350  0.009890  0.003316   \n",
      "24  ...  0.015833  0.011210  0.006422  0.004342  0.009696  0.009439  0.005264   \n",
      "25  ...  0.013183  0.010282  0.004928  0.000988  0.010293  0.011563  0.002570   \n",
      "26  ...  0.005220  0.009425  0.009831  0.004372  0.006279  0.008769  0.008341   \n",
      "27  ...  0.004135  0.006314  0.036601  0.000751  0.007021  0.009111  0.003527   \n",
      "28  ...  0.004329  0.010446  0.033090  0.000271  0.009675  0.012573  0.003040   \n",
      "29  ...  0.005417  0.028128  0.022890  0.000489  0.013516  0.007306  0.001973   \n",
      "30  ...  0.003436  0.037098  0.016478  0.000379  0.009610  0.005793  0.002298   \n",
      "31  ...  0.007539  0.021443  0.012170  0.000633  0.012059  0.010056  0.002396   \n",
      "32  ...  0.005503  0.009040  0.024327  0.001404  0.007845  0.012957  0.004887   \n",
      "33  ...  0.005764  0.013391  0.013472  0.002203  0.008463  0.008969  0.005799   \n",
      "34  ...  0.005169  0.005438  0.012521  0.002333  0.005277  0.013087  0.007620   \n",
      "35  ...  0.007775  0.009696  0.006805  0.002765  0.007766  0.011405  0.006948   \n",
      "\n",
      "     yeofly1   yercac1    ywcpar  \n",
      "0   0.013386  0.007358  0.004230  \n",
      "1   0.013384  0.007266  0.003798  \n",
      "2   0.009186  0.007488  0.004560  \n",
      "3   0.011040  0.007817  0.003300  \n",
      "4   0.011366  0.005008  0.004432  \n",
      "5   0.014030  0.007142  0.003684  \n",
      "6   0.014280  0.007154  0.003804  \n",
      "7   0.018396  0.006584  0.003247  \n",
      "8   0.014468  0.006567  0.003536  \n",
      "9   0.014583  0.007365  0.003622  \n",
      "10  0.012892  0.007328  0.004015  \n",
      "11  0.012006  0.007138  0.003523  \n",
      "12  0.081227  0.008177  0.001490  \n",
      "13  0.061634  0.008733  0.003084  \n",
      "14  0.038078  0.010809  0.002722  \n",
      "15  0.046365  0.014049  0.001839  \n",
      "16  0.076075  0.011017  0.001343  \n",
      "17  0.034368  0.013382  0.002703  \n",
      "18  0.033845  0.010240  0.002679  \n",
      "19  0.055968  0.010529  0.002544  \n",
      "20  0.039523  0.013534  0.002220  \n",
      "21  0.049062  0.010717  0.002335  \n",
      "22  0.067941  0.008733  0.001594  \n",
      "23  0.023671  0.008980  0.003956  \n",
      "24  0.034235  0.012325  0.002745  \n",
      "25  0.052106  0.012568  0.001112  \n",
      "26  0.012054  0.015146  0.002615  \n",
      "27  0.007257  0.028137  0.003197  \n",
      "28  0.007950  0.024120  0.001580  \n",
      "29  0.018990  0.013934  0.001542  \n",
      "30  0.011278  0.019111  0.001231  \n",
      "31  0.031609  0.015383  0.001250  \n",
      "32  0.012492  0.019304  0.002519  \n",
      "33  0.015503  0.016943  0.002241  \n",
      "34  0.009585  0.020628  0.002226  \n",
      "35  0.019925  0.017465  0.001921  \n",
      "\n",
      "[36 rows x 207 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import sys\n",
    "# sys.path.append('/kaggle/input/bird-clef-utils')\n",
    "from utils import EffNetB0Classifier, process_spectrogram\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "label_map = json.load(open('label_map.json'))\n",
    "# Class labels from train audio\n",
    "class_labels = sorted(list(label_map.keys()))\n",
    "\n",
    "# List of test soundscapes (only visible during submission)\n",
    "# test_soundscape_path = '/kaggle/input/birdclef-2025/train_soundscapes'\n",
    "test_soundscape_path = 'train_soundscapes'\n",
    "test_soundscapes = [os.path.join(test_soundscape_path, afile) for afile in sorted(os.listdir(test_soundscape_path)) if afile.endswith('.ogg')]\n",
    "\n",
    "# Open each soundscape and make predictions for 5-second segments\n",
    "# Use pandas df with 'row_id' plus class labels as columns\n",
    "predictions = pd.DataFrame(columns=['row_id'] + class_labels)\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "model = EffNetB0Classifier(num_classes=206)\n",
    "model.load_state_dict(torch.load(\"my_model.pth\", map_location=torch.device('cpu'), weights_only=True))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "for soundscape in test_soundscapes[:3]:\n",
    "    # Load audio\n",
    "    sig, rate = librosa.load(path=soundscape, sr=None)\n",
    "\n",
    "    # Split into 5-second chunks\n",
    "    chunks = []\n",
    "    for i in range(0, len(sig), rate * 5):\n",
    "        chunk = sig[i:i + rate * 5]\n",
    "        if len(chunk) < rate * 5:  # Pad the chunk if it's less than 5 seconds\n",
    "            chunk = np.pad(chunk, (0, rate * 5 - len(chunk)), mode='constant')\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    # Make predictions for each chunk\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Compute the spectrogram\n",
    "        spectrogram = librosa.feature.melspectrogram(y=chunk, sr=rate, n_mels=128)\n",
    "        spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "        spectrogram_tensor = process_spectrogram(spectrogram_db)\n",
    "        \n",
    "        # Get row id (soundscape id + end time of 5s chunk)\n",
    "        row_id = os.path.basename(soundscape).split('.')[0] + f'_{i * 5 + 5}'\n",
    "\n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = model(spectrogram_tensor.unsqueeze(0))\n",
    "            scores = torch.softmax(outputs, dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "        # Append to predictions as new row\n",
    "        new_row = pd.DataFrame([[row_id] + list(scores)], columns=['row_id'] + class_labels)\n",
    "        predictions = pd.concat([predictions, new_row], axis=0, ignore_index=True)\n",
    "\n",
    "# Save prediction as csv\n",
    "predictions.to_csv('submission.csv', index=False)\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "sourceId": 91844,
     "sourceType": "competition"
    },
    {
     "datasetId": 7305593,
     "sourceId": 11647876,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 324619,
     "modelInstanceId": 304144,
     "sourceId": 366843,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 324619,
     "modelInstanceId": 304144,
     "sourceId": 368061,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 324619,
     "modelInstanceId": 304144,
     "sourceId": 368487,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 324619,
     "modelInstanceId": 304144,
     "sourceId": 369278,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "birdclef-2025-YT0uVT6V-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17.855052,
   "end_time": "2025-05-02T08:05:53.106156",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-02T08:05:35.251104",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
